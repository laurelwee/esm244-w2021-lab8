---
title: "EMS 244 Lab Week 8: Clustering (k-means & hierarchical)"
author: "Laurel Wee"
date: "3/4/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```
```{r}
# Load packages
pacman::p_load(tidyverse,here,janitor,palmerpenguins)
# Packages for cluster analysis
pacman::p_load(NbClust,cluster,factoextra,dendextend,ggdendro)
```

### Introduction to cluster analysis (k-means, hierachical)
To practice k-means clustering we will use the `penguins` dataset from `palmerpenguins`

#### Part 1. K-means clustering:
First, do exploratory data vizualiztion, mapping species onto point color. Does it look like there is an opportunity to cluster by species?

```{r}
# Bill length vs depth exploratory plot:
ggplot(penguins)+
  geom_point(aes(x=bill_length_mm,
                 y=bill_depth_mm,
                 color=species,
                 shape=sex),
             size=3,
             alpha=0.7)+
  scale_color_manual(values = c("orange","cyan4","darkmagenta"))
```
```{r}
# Flipper length vs body mass exploratory plot:
ggplot(penguins)+
  geom_point(aes(x=flipper_length_mm,
                 y=body_mass_g,
                 color=species,
                 shape=sex),
             size= 3,
             alpha=0.7)+
  scale_color_manual(values = c("orange","cyan4","darkmagenta"))
```

#### Pick the number of clusters
```{r}
# How many clusters do you think there should be?
number_est <- NbClust(penguins[3:6], min.nc = 2, max.nc = 10, method = "kmeans")
```

```{r}
# Check out the results (just look at the first summary report):
number_est

# By these estimators, 2 is identified as the best number of cluisters by the largest number of algorithms (8/30), but should that change our mind? Maybe, but here it makes sense to still stick with 3 (a cluster for each species) and see how it does.
```
### Create a complete, sclaed version of the data

```{r}
# Drop rows where any of the four size measurements are missing
penguins_complete <- penguins %>% 
  drop_na(bill_length_mm, bill_depth_mm, body_mass_g, flipper_length_mm)

# Only keeps the columns for the four size measurements, the scale them
penguins_scale <- penguins_complete %>% 
  select(ends_with("mm"),body_mass_g) %>% 
  scale()
penguins_scale
```

### Run k-means

```{r}
penguins_km <- kmeans(penguins_scale, 3) # kmeans specifying 3 groups to start
penguins_km

# See what it returns (different elements returned by kmeans function):
penguins_km$size # How many observations assigned to each cluster

penguins_km$cluster # What cluster each observation in penguins_scale is assigned to
```

```{r}
# Bind the cluster number to the original data used for clustering, so that we can see what cluster each penguin is assigned to
penguins_cl <- data.frame(penguins_complete,cluster_no = factor(penguins_km$cluster))

# Plot flipper length versus body mass, indicating which cluster each penguin is assigned to (but also showing the actual species):
ggplot(penguins_cl)+
  geom_point(aes(x=flipper_length_mm,
                 y=body_mass_g,
                 color=cluster_no,
                 shape=species))

```

```{r}
ggplot(penguins_cl)+
  geom_point(aes(x=bill_length_mm,
                 y=body_mass_g,
                 color=cluster_no,
                 shape=species))

# A lot of Gentoos in Cluster 3, Chinstrap in cluster 1, and Adelie in cluster 2
```
```{r}
# Find the counts of each species assigned to each cluster, then pivot_wider() to make it a contingency table:
penguins_cl %>% 
  count(species, cluster_no) %>% 
  pivot_wider(names_from = cluster_no, values_from = n) %>% 
  rename('Cluster 1' = '1', 'Cluster 2' = '2', 'Cluster 3' = '3')
```
Takeaway: as we see from the graph, most chinstraps in Cluster 1, and most Adelies in Cluster 2, and all Gentoos are in Cluster 3 by k-means clustering. So this actually does a somewhat decent job of splitting up the three species into different clusters, with some overlap in Cluster 1 between Adelies & chinstraps, which is consistent with what we observed in exploratory data visualization.

### Part 2. Cluster analysis: hierarchical
In this section, you’ll be performing hierarchical cluster analysis (& making dendrograms) in R. We will use the stats::hclust() function for agglomerative hierarchical clustering, using WorldBank environmental data (simplified), wb_env.csv.

#### Read in the data & simplify
Here, we’ll read in the WorldBank environmental data (simplified), and keep only the top 20 GHG emitters for this dataset.

```{r}
# Get the data
wb_env <- read_csv("wb_env.csv")

#View(wb_env)

# Only keep top 20 GHG emitters (for simplifying visualization)
wb_ghg_20 <- wb_env %>% 
  arrange(-ghg) %>% 
  head(20)
```

#### Scale the data
```{r}
# Scale the numeric variable (columns 3:7)
wb_scaled <- wb_ghg_20 %>% 
  select(3:7) %>% 
  scale()

# Update to add rownames (country name) from wb_ghg_20
rownames(wb_scaled)<- wb_ghg_20$name

# Check the outcome with View(wb_scaled) - see that the rownames are now the country name (this is useful for visualizing)
```

Great, now we have a simplified, scaled version of the numeric variables, with rownames containing the county name.

#### Find the Euclidean distances
Use the stats::dist() function to find the Euclidean distance in multivariate space between the different observations (countries):

```{r}
# Compute dissimilarity values (Euclidean distances):
euc_distance <-dist(wb_scaled, method = "euclidean")
#View(euc_distance)

```

#### Perform hierarchical Clustering by complete linkage with `stats::hclust()`
The `stats::hclust()` function performs hierarchical clustering, given a dissimilarity matrix (our matrix of euclidean distances), using a linkage that you specify. 

Here, let's use complete linkage (recall from lecture: clusters are merged by the smallest *maximum* distance between two observations in distinct clusters).

```{r}
# Hierarchical clustering (complete linkage)
hc_complete <- hclust(euc_distance, method = "complete" )

# Plot it (base plot):
plot(hc_complete, cex = 0.6, hang = -1)
```
#### Now do it by single linkage & compare
Let’s update the linkage to single linkage (recall from lecture: this means that clusters are merged by the smallest distance between observations in separate clusters):
```{r}
# Hierarchical clustering (single linkage)
hc_single <- hclust(euc_distance, method = "single" )

# Plot it (base plot):
plot(hc_single, cex = 0.6, hang = -1)
```

We see that it is a bit different when we change the linkage! But how different? 

#### Make a tanglegram to compare dendrograms 

Let's make a **tanglegram** to compare clustering by complete and single linkage! We'll use the `dendextend::tanglegram()` function to make it. 

First, we'll convert to class `dendrogram`, then combine them into a list:

```{r}

# Convert to class dendrogram
dend_complete <- as.dendrogram(hc_complete)
dend_simple <- as.dendrogram(hc_single)
```

Cool, now make a tanglegram: 

```{r}
# Make a tanglegram
tanglegram(dend_complete, dend_simple)
```

That allows us to compare how things are clustered by the different linkages!

#### Want to plot your dendrogram with ggplot instead? Me too. 

Here's how you can make your dendrogram with `ggplot` (here, I'll use the complete linkage example stored as `hc_complete`) using `ggdendrogram()`, a `ggplot` wrapper: 

```{r}
ggdendrogram(hc_complete, 
             rotate = TRUE) +
  theme_minimal() +
  labs(x = "Country")

# COOL. Then you can customize w/ usual ggplot tools. 
```

## End Week 8 lab
